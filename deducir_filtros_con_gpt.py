from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def analizar_intencion_con_gpt(pregunta: str) -> dict:
    """
    Analiza la intenci√≥n de una pregunta y devuelve filtros + tokens usados
    """
    system_prompt = """Sos un asistente que ayuda a traducir una pregunta de un usuario sobre sus notas personales a filtros para b√∫squeda en una base de datos.

Dado el texto de una pregunta, devolv√©s un JSON con alguno de los siguientes campos si se pueden deducir:
- fecha (formato YYYY-MM-DD)
- fecha_hasta (opcional, para rango)
- emocion (ej: "Triste", "Feliz")
- categoria (ej: "Trabajo", "Salud")
- tags (lista de palabras clave)
- ubicacion (ej: "Recoleta", "Oficina")
- top_k (int, cu√°ntas notas se deben buscar)

Solo devolv√©s campos si hay una inferencia clara. Si no hay nada claro, devolv√© solo {"top_k": 15}.
"""

    user_prompt = f"Texto de la pregunta: {pregunta}\n\nRespond√© solo con el JSON de filtros."

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )
        contenido = response.choices[0].message.content.strip()

        # Intentar parsear como JSON
        import json
        filtros = json.loads(contenido)
        
        if isinstance(filtros, dict):
            return {
                "filtros": filtros,
                "top_k": filtros.get("top_k", 15),
                "tokens_usados": response.usage.total_tokens  # üìä Agregar tokens
            }
        else:
            return {
                "filtros": {},
                "top_k": 15,
                "tokens_usados": response.usage.total_tokens
            }
    except Exception as e:
        print(f"Error al deducir filtros con GPT: {e}")
        return {
            "filtros": {},
            "top_k": 15,
            "tokens_usados": 0
        }